---
title: "Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework(2024)"
date: 2025-04-25
categories: [Multi-Agent]
tags: [Multi-Agent, LLM, Hallucination]
mathjax: true
---
요약: LLM 환각 탐지의 정확도를 높이기 위해, 본 논문은 Trust-Skeptic-Leader 세 에이전트가 직전 검증 결과(True/False)에 따라 토론 흐름을 전환하는 마르코프 체인 기반의 동적 토론 구조를 도입
1️⃣ LLM 응답 ➔ Claim 단위로 분해
2️⃣ Claim 기반 쿼리 생성 ➔ Evidence 수집
3️⃣ 다중 에이전트 토론을 통해 검증

이는 인간 토론 방식을 모사해 적응적이고 심층적인 검증을 가능하게 함
별도의 학습 없이 블랙박스 LLM 환경에서 효과적으로 환각을 탐지하는 새로운 사후(post-processing) 프레임워크로서의 노벨티



### 1️⃣ 문제 정의 & 연구 동기

**■ 문제(Problem Statement)**

LLM이 생성하는 **환각(Hallucination)** 문제를 효과적으로 탐지하는 방법 제안. 기존 방식들은 주로 학습 단계 개입 또는 단순한 사후 검증에 의존.

**■ 왜 중요한가?**

- LLM의 확산으로 인해 잘못된 정보 생성이 실제 서비스와 산업적 활용에서 심각한 문제로 부각됨.
- ChatGPT, GPT-4 등 **블랙박스 모델**의 환각 탐지는 특히 어렵고, 고비용의 학습 개입이 현실적으로 어렵기 때문.
- 학문적으로는 범용적이고 효율적인 **사후(Post-processing) 환각 탐지** 방법의 필요성이 대두.

**■ 기존 연구(Previous Work)**

- 학습 단계 개입 방식 (pre-training, fine-tuning, alignment 등) → 비용/복잡도 문제.
- 사후 처리 방식 → 주로 **Claim Detection**과 **Evidence Retrieval** 중심, 검증(Verification) 과정은 단순 프롬프트 기반.
- **Multi-agent debate** 활용 시도는 있었으나, 대부분 reasoning 향상 목적이었고 환각 탐지에 직접 적용된 사례는 드뭄.

**■ 기존 한계점**

- 검증 과정의 정확도 부족: 단순한 단일 에이전트 판단에 의존.
- 인간 토론처럼 유연하게 진행되지 않는 **고정형 디베이트 구조**.
- 블랙박스 LLM 환경에서 적용하기 어려운 학습 기반 접근법.

---

### 2️⃣ 제안 방법 (Proposed Method)

**■ 해결 방안**

- **Markov Chain 기반 Multi-agent Debate Verification Framework** 제안.
- 3단계 프로세스:
    1. Claim Detection(주장탐지)
        1. LLM의 응답에서, 세부 검증 단위인 claim을 gpt로 추출
    2. Evidence Retrieval
        1. claim에서 gpt가 queries생성 → 내부 지식/google API 통해 증거 검색
    3. Multi-agent Verification (다중 에이전트 검증)
        1. 1st agent가 사실인가? → 2nd agent가 동의/반대 → 최종 agent가 종합 판단: 사실인지 환각인지

**■ 핵심 아이디어**

- 에이전트(Trust, Skeptic, Leader) 간 **마코프 체인 형태의 유연한 토론 구조** 도입.
    - 인간처럼 "긍정 ↔ 의심 ↔ 종합" 과정을 통해 **더 신중한 판단**을 유도.
    - 만약 단순히 2가지(T/F) 역할만 있었다면, 깊이 있는 검증이 어렵고, 논리적 보완이 부족했을 것
- 이전 상태 결과에 따라 토론 모드를 전환하며 최적의 검증 결과 도출.
- 인간의 토론 방식을 모사하여 검증 과정의 **정확성과 적응성** 향상.

**■ 기존 방법과 차별점**

- 단일 에이전트 검증 ➔ **다중 에이전트 동적 검증**
- 고정된 디베이트 흐름 ➔ **상태 전이에 따른 유연한 검증 과정**
- LLM 블랙박스 환경에서도 적용 가능 (학습 불필요, 프롬프트 기반)

**■ 복잡도**

- 계산량: LLM API 호출이 반복되므로 **비용과 시간적 오버헤드** 존재.
- 구현 난이도: 프롬프트 설계 및 상태 전이 로직 필요.
- 실시간 서비스 적용에는 부담.

**■ 적용 범위**

- QA, Summarization, Dialogue 등 다양한 생성 작업에 범용 적용 가능.
- 블랙박스 LLM 환경에서의 **사후 환각 탐지/완화(post-processing mitigation)**.

---

### 3️⃣ 실험 (Experiments)

**■ 데이터셋**

- **공개 데이터셋** 사용.
    - [**Factool**](https://github.com/GAIR-NLP/factool): ChatGPT가 생성한 응답과 이에 대한 claim이 주어진 데이터셋.
    - [**HaluEval**](https://github.com/RUCAIBox/HaluEval): LLM의 환각 사례를 포함한 대규모 평가용 데이터셋으로, QA, Summarization, Dialogue 태스크로 구성됨.
- 두 데이터셋 모두 **환각 여부가 라벨링**된 상태로 제공되어 평가에 적합.
- QA, Summarization, Dialogue 3가지 태스크.

**■ 실험 세팅**

- 사용 LLM: **ChatGPT (GPT-3.5-turbo, GPT-4)**
- 주요 설정:
    - **Evidence Retrieval**: 최대 10개의 증거 수집.
    - **Multi-agent Debate**: 최소 2회 디베이트 진행.
    - **상태 전이 방식**: 직전 결과가 True일 경우 Skeptic 모드로 전환 (**True → Skeptic**).
    - **종료 조건**: 3명의 에이전트(Trust, Skeptic, Leader)가 동일한 판단을 내릴 때 종료.
- 환경: LLM API 호출 기반으로 실험 진행.

**■ 비교 대상(Baseline)**

기존 환각 탐지 기법과 비교 수행.

- **Self-check**: LLM이 스스로 생성물을 검증.
- **Factool**: 증거 기반 단일 검증 프로세스.
- **HaluEval few-shot prompting**: 간단한 프롬프트 기반 검증.
- **WeCheck**: 비-GPT 기반 약지도 학습(weak supervision) 방식.

**■ Ablation Study**

- **전이 방식 실험**:
    
    다양한 상태 전이 전략(Always Trust, Always Skeptic, True→Trust 등) 비교.
    
    ➔ **True → Skeptic** 방식이 가장 높은 정확도와 정밀도 기록.
    
- **디베이트 최소 라운드 실험**:
    
    0~3회까지 라운드 수 변화에 따른 성능 분석.
    
    ➔ **1~2회**에서 최적 성능, 3회 이상 시 과도한 의심으로 성능 저하.
    
- **비GPT 방식 비교**:
    
    WeCheck와 비교하여 multi-agent debate 구조의 효과를 입증.
    

**■ 결과 해석 및 인사이트**

- 대부분의 지표에서 기존 방식 대비 **정확도(Accuracy)** 향상.
- Recall은 낮지만 Precision이 높아, **오탐보다 미탐 방지**에 초점.
- 더 큰 LLM(GPT-4) 사용 시 reasoning 향상 효과 확인.
- 디베이트 라운드 증가가 모든 경우에 긍정적이지는 않음.

**■ 실험 결과를 어떻게 해석하고, 어떤 인사이트를 도출했는가?**

- **유연한 토론 구조**가 단일 검증 방식 대비 **더 신중하고 정확한 판단**을 유도.
- Skeptic 중심의 반복 검토가 잠재적 오류를 효과적으로 탐지.
- Recall이 다소 낮은 이유는, **환각을 과소탐지하기보다는 과잉탐지(보수적 판단)**하는 설계 때문.
- GPT-4 사용 시 reasoning 성능이 높아져 debate 방식의 효과가 더욱 극대화됨.
- **디베이트 라운드 최적화**가 중요하며, 무조건 반복이 성능 향상으로 이어지지 않음.

**■ 실험이 재현 가능한가? (코드/데이터 공개 여부)**

- 데이터셋은 모두 **공개**.
- 프롬프트 설계와 전체 절차는 논문 부록에 상세히 기술되어 있어 원리 이해 및 구현 가능.
- 하지만 상용 LLM API 호출이 필수이므로, **비용과 호출 제한**으로 인해 실험 재현에는 현실적 제약이 존재.
- 코드 전체 공개 여부는 논문에서 언급되지 않음.

---

### 4️⃣ 논문의 기여도 (Contributions & Novelty)

**■ 주요 기여 (3줄 요약)**

1. 다양한 생성 태스크에 적용 가능한 **범용 환각 탐지 프레임워크** 제안.
2. **Markov Chain 기반 유연한 Multi-agent Debate** 구조 설계.
3. 기존 방법 대비 높은 검증 정확도를 실험적으로 입증.

**■ Novelty**

- 인간 토론 방식을 모사한 **동적 상태 전이 디베이트**를 환각 탐지에 최초로 적용.
- 블랙박스 LLM 환경에서 별도의 학습 없이 성능을 향상시킨 접근.

**■ 임팩트**

- **단기적**: 사후 환각 탐지 정확도 개선, 다양한 태스크에 활용 가능.
- **장기적**: LLM 신뢰성 확보를 위한 **패러다임 전환** 가능성, 인간-에이전트 상호작용 기반 검증 프레임워크 확장.

---

### 제안된 방법: 마르코프 체인 기반 다중 에이전트 토론 검증 프레임워크

논문은 환각 탐지를 위한 사실 확인(fact-checking) 프로세스를 제안하며, 이를 세 단계로 나눕니다: 주장 탐지, 증거 검색, 다중 에이전트 검증. 특히, 다중 에이전트 검증 단계에서 마르코프 체인(Markov Chain)을 활용한 새로운 토론 프레임워크를 도입합니다.

### 2.1 주장 탐지(Claim Detection)

이 단계에서는 대형 언어 모델(예: ChatGPT)을 활용하여 복잡한 응답에서 개별 주장을 추출합니다. 예를 들어, 긴 문장에서 사실 확인이 가능한 단위로 분해하는 과정입니다. 이는 복잡한 문제를 더 작은 단위로 나누어 분석 가능하게 만듭니다.

**결과**: 주장 단위로 분해된 명확한 검증 대상.

### 2.2 증거 검색(Evidence Retrieval)

추출된 주장에 대해 관련 증거를 수집합니다. ChatGPT를 사용해 주장과 관련된 질의를 생성하고, Google API나 제공된 지식 데이터를 통해 증거를 검색합니다. 이는 주장의 사실 여부를 판단할 자료를 확보하는 과정입니다.

**결과**: 주장 검증을 위한 신뢰할 수 있는 증거 데이터.

### 2.3 다중 에이전트 검증(Multi-agent Verification)

이 단계는 논문의 핵심 기여로, 마르코프 체인을 기반으로 한 다중 에이전트 토론 프레임워크를 통해 주장을 검증합니다. 세 가지 역할의 에이전트(Trust, Skeptic, Leader)를 정의하고, 이들이 상호작용하며 주장을 평가합니다.

- **Trust 에이전트**: 이전 에이전트의 의견을 신뢰하며 이를 바탕으로 주장을 긍정적으로 검토합니다.
- **Skeptic 에이전트**: 이전 에이전트의 의견에 의문을 제기하며 증거와의 불일치를 찾습니다.
- **Leader 에이전트**: Trust와 Skeptic의 의견을 종합하여 최종 판단을 내립니다.

이 프레임워크는 마르코프 체인의 상태 전이(state transition)를 활용합니다. 두 가지 토론 모드(Trust-Skeptic-Leader, Skeptic-Trust-Leader)가 정의되며, 이전 상태의 판단 결과에 따라 다음 상태로 전이됩니다. 예를 들어, 주장이 사실로 판단되면 Skeptic 모드로 전이하여 추가 검증을 수행하고, 비사실로 판단되면 Trust 모드로 전이하여 신뢰성을 재검토합니다. 토론은 에이전트들이 합의에 도달하거나 최대 라운드에 도달할 때 종료됩니다.

**결과**: 인간의 토론 과정을 모방한 유연하고 정확한 주장 검증.